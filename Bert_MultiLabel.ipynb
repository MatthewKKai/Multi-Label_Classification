{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BertForSeqClassification\n",
    "## define bert model, optmizer, loss func etc.\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 672)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# the inserted loss function for BertForSeqClassification, here we replace with BCELoss multi-label\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "# optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and process for train\n",
    "data = pd.read_csv(r\"data_with_labels.csv\", encoding = \"utf-8\", error_bad_lines = False)\n",
    "def load_data(data, i):\n",
    "    text = data.iloc[i][\"text\"]\n",
    "    labels = data.iloc[i][1:]\n",
    "    return text, labels\n",
    "def tokenize(tokenizer, text):\n",
    "    tokens = tokenizer(text, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::  14%|█▍        | 3053/21578 [3:03:51<18:35:36,  3.61s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9cda37e5e547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mphbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# train\n",
    "with tqdm(total = len(data)) as phbar:\n",
    "    phbar.set_description(\"Processing:\")\n",
    "    for i in range(len(data)):\n",
    "        text, labels = load_data(data, i)\n",
    "        input_ids, attention_mask = tokenize(tokenizer, text)\n",
    "        optimizer.zero_grad()\n",
    "        labels = torch.tensor(labels).unsqueeze(0)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        try:\n",
    "            loss = loss_func(outputs.logits, labels.fl)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        phbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:: 100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,recall_score, precision_score\n",
    "# test\n",
    "total = 0\n",
    "correct = 0\n",
    "recall_micro = 0\n",
    "recall_macro = 0\n",
    "precision_micro = 0\n",
    "precision_macro = 0\n",
    "micro_f1 = 0\n",
    "macro_f1 = 0\n",
    "with tqdm(total = 500) as phbar:\n",
    "    phbar.set_description(\"Processing:\")\n",
    "    for i in range(len(data) - 500, len(data)):\n",
    "        text, labels = load_data(data, i)\n",
    "        input_ids, attention_mask = tokenize(tokenizer, text)\n",
    "        labels = torch.tensor(labels).unsqueeze(0)\n",
    "        # generate prediction\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)  # don't use internal CrossEntropyLoss\n",
    "        prob = outputs.logits.sigmoid()   # Because BCEWithLogitsLoss has sigmoid\n",
    "            \n",
    "        # record processed data count\n",
    "        total += (labels.size(0)*labels.size(1))\n",
    "\n",
    "        # take the index of the highest prob as prediction output\n",
    "        THRESHOLD = 0.7\n",
    "        prediction = prob.detach().clone()\n",
    "        prediction[prediction > THRESHOLD] = 1\n",
    "        prediction[prediction <= THRESHOLD] = 0\n",
    "        recall_micro += recall_score(prediction, labels, average=\"micro\", zero_division=1)\n",
    "        precision_micro += precision_score(prediction, labels, average=\"micro\", zero_division=1)\n",
    "        recall_macro += recall_score(prediction, labels, average=\"macro\", zero_division=1)\n",
    "        precision_macro += precision_score(prediction, labels, average=\"macro\", zero_division=1)\n",
    "        micro_f1 += f1_score(prediction, labels, average=\"micro\", zero_division=1)\n",
    "        macr0_f1 += f1_score(prediction, labels, average=\"macro\",zero_division=1)\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "        phbar.update(1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 335292  / total: 336000 / test_acc: 99.789286 / test_recall_micro: 0.088690 / test_recall_macro: 0.148720 / test_precision_micro: 0.077103 / test_precision_macro: 0.148585 / test_micro_f1: 0.065266 / test_macro_f1: 0.381079\n"
     ]
    }
   ],
   "source": [
    "# print completed result\n",
    "acc = 100.*correct/total\n",
    "recall_micro = 100.*recall_micro/total\n",
    "recall_macro = 100.*recall_macro/total\n",
    "precision_micro = 100.*precision_micro/total\n",
    "precision_macro = 100.*precision_macro/total\n",
    "micro_f1 = 100.*micro_f1/total\n",
    "macro_f1 = 100*macr0_f1/total\n",
    "\n",
    "print('correct: %i  / total: %i / test_acc: %f / test_recall_micro: %f / test_recall_macro: %f / test_precision_micro: %f / test_precision_macro: %f / test_micro_f1: %f / test_macro_f1: %f' % (correct, total, acc, recall_micro, recall_macro, precision_micro, precision_macro, micro_f1, macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
